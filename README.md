# ğŸ§µ Fashion MNIST: PCA & Dimensionality Reduction Analysis  

## ğŸš€ Project Overview  
This project explores **dimensionality reduction techniques** applied to the **Fashion-MNIST dataset**, a popular benchmark dataset for image classification. The goal is to reduce the datasetâ€™s **high-dimensional space** into **2D representations**, enabling better visualization and analysis while retaining meaningful information.  

We employ **Principal Component Analysis (PCA)** and compare it with an **alternative dimensionality reduction technique** to evaluate their effectiveness in terms of **variance retention** and **class separability**.

## ğŸ¯ Objectives  
âœ”ï¸ Apply **PCA** (Principal Component Analysis) to project high-dimensional image data into **2D space**.  
âœ”ï¸ Visualize results using **scatter plots**, with distinct colors for each class.  
âœ”ï¸ Compare PCA with another **dimensionality reduction technique** (e.g., **t-SNE, LDA, or Autoencoders**).  
âœ”ï¸ Analyze **variance ratio retention** to determine the best method.  
âœ”ï¸ Discuss real-world applications of dimensionality reduction in **machine learning and AI**.

---

## ğŸ›  Dataset: Fashion MNIST  
- **60,000 training images** + **10,000 test images**.  
- **28Ã—28 grayscale images** per sample.  
- **10 categories** representing different clothing items (e.g., T-shirt, Sneakers, Dress, etc.).  
- **Used in image classification & deep learning research** as a **drop-in replacement** for the MNIST digit dataset.  

ğŸ“Œ **Dataset Reference:** [Fashion-MNIST GitHub](https://github.com/zalandoresearch/fashion-mnist)  

---

## ğŸ“Š Methods & Techniques Used  

### ğŸ”¹ **1ï¸âƒ£ Principal Component Analysis (PCA)**
- Reduces dataset dimensions to **2D**.
- **Eigenvalue decomposition** is used to retain maximum variance.
- **Explained variance ratio** is computed to measure retained information.
- Generates **scatter plots** for visualization.

### ğŸ”¹ **2ï¸âƒ£ Alternative Dimensionality Reduction Method**
- Implements another technique (**t-SNE, LDA, Autoencoders, or UMAP**).
- Compares **variance retention and class separability**.
- Visualizes results using **scatter plots**.

### ğŸ”¹ **3ï¸âƒ£ Conceptual Analysis**
- Compares variance retention between both methods.
- Evaluates which method provides **better class separation**.
- Discusses real-world implications in **AI, deep learning, and big data**.

---
